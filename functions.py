import numpy as np
import pandas as pd
import streamlit as st
import matplotlib.pyplot as plt
import networkx as nx
import io
import zipfile
import unicodedata

### ìë™í™” ê´€ë ¨ í•¨ìˆ˜ ì„ ì–¸
def _nfc(s: str) -> str:
    return unicodedata.normalize('NFC', s)

def _fix_zip_name(name: str) -> str:
    """
    zipfileì´ cp437ë¡œ ì˜ëª» ë””ì½”ë”©í•œ íŒŒì¼ëª…ì„ ë³µêµ¬ ì‹œë„.
    1) cp437 bytesë¡œ ë˜ëŒë¦° ë’¤
    2) utf-8 / cp949 ìˆœìœ¼ë¡œ decode ì‹œë„
    """
    try:
        raw = name.encode("cp437")
    except Exception:
        return name

    for enc in ("utf-8", "cp949"):
        try:
            return raw.decode(enc)
        except Exception:
            pass

    # ìµœí›„: cp949ë¡œ ê¹¨ì§€ë”ë¼ë„ replace
    return raw.decode("cp949", errors="replace")


def _pick_excel_from_zip(z: zipfile.ZipFile, original_filename_no_ext: str):
    """ZIP ë‚´ë¶€ì—ì„œ ì›ë³¸ íŒŒì¼ëª… ê¸°ë°˜ ë§¤ì¹­ -> ì‹¤íŒ¨ ì‹œ ì²« ë²ˆì§¸ ì—‘ì…€ fallback"""

    infos = []
    for info in z.infolist():
        raw = info.filename
        fixed = _nfc(_fix_zip_name(raw)).replace("\\", "/")

        # __MACOSX ì œê±° + ì—‘ì…€ë§Œ
        if fixed.startswith("__MACOSX") or "/__MACOSX/" in fixed:
            continue
        if not fixed.endswith((".xlsx", ".xls")):
            continue

        infos.append((info, fixed))

    # (í‘œì‹œìš©) clean name
    clean_names = []
    info_by_clean = {}
    for info, fixed in infos:
        base = fixed.split("/")[-1]
        clean_no_ext = base.rsplit(".", 1)[0]
        clean_names.append(clean_no_ext)
        info_by_clean[clean_no_ext] = info   # âœ… ZipInfo ì €ì¥

    # 1) ìë™ ë§¤ì¹­
    norm_orig = _nfc(original_filename_no_ext)
    for clean in clean_names:
        parts = [x for x in clean.split("_") if x]
        parts = [_nfc(x) for x in parts]
        if parts and all(part in norm_orig for part in parts):
            return clean, info_by_clean[clean], "matched"

    # 2) fallback: ì²« ë²ˆì§¸ ì—‘ì…€
    if clean_names:
        clean = clean_names[0]
        return clean, info_by_clean[clean], "fallback_first"

    return None, None, "no_excel"



def prepare_batch_preview(alpha_file, original_filename_no_ext: str):
    """
    1) ZIPì´ë©´ ë§¤ì¹­ í›„ batch_df ë¡œë“œ / ì—‘ì…€ì´ë©´ ë°”ë¡œ ë¡œë“œ
    2) í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸° ë¼ì¸ ìƒì„±
    return: (batch_df, meta, preview_lines, summary_lines)
    """
    meta = {
        "uploaded": alpha_file.name,
        "kind": "zip" if alpha_file.name.endswith(".zip") else "excel",
        "matched_file": None,
        "match_mode": None
    }

    # --- 1ë‹¨ê³„: íŒŒì¼ í™•ë³´ (ì—…ë¡œë“œ ì¦‰ì‹œ ì‹¤í–‰) ---
    if alpha_file.name.endswith(".zip"):
        zip_bytes = io.BytesIO(alpha_file.getvalue())
        with zipfile.ZipFile(zip_bytes, 'r') as z:
            matched_clean, matched_info, mode = _pick_excel_from_zip(z, original_filename_no_ext)
            if mode == "no_excel":
                raise ValueError("ZIP ë‚´ë¶€ì— ì—‘ì…€ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

            meta["matched_file"] = matched_clean
            meta["match_mode"] = mode

            # âœ… ë¬¸ìì—´ ê²½ë¡œê°€ ì•„ë‹ˆë¼ ZipInfoë¡œ open
            with z.open(matched_info) as f:
                batch_df = pd.read_excel(
                f,
                dtype=str  # <â”€ ì „ì²´ë¥¼ ë¬¸ìì—´ë¡œ ë°›ìŒ (ìˆ«ìë¡œ ì˜¤ì¸ ê¸ˆì§€)
            )

    else:
        meta["matched_file"] = alpha_file.name
        meta["match_mode"] = "no_match_needed"
        batch_df = pd.read_excel(
            alpha_file,
            dtype=str  # <â”€ ì—¬ê¸°ì„œë„ ë™ì¼
        )

    # --- ê²€ì¦/ì •ë¦¬ ---
    needed_cols = {"from", "to", "to_name", "alpha"}
    if not needed_cols.issubset(batch_df.columns):
        raise ValueError(f"ì—‘ì…€ íŒŒì¼ì— ë‹¤ìŒ ì»¬ëŸ¼ì´ í¬í•¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤: {needed_cols}")


    df = batch_df.copy()
    df["from"] = df["from"].astype(str)
    df["to"] = df["to"].astype(str)
    df["to_name"] = df["to_name"].astype(str)
    df["to_name"] = df["to_name"].replace("nan", "").fillna("")
    df["alpha"] = pd.to_numeric(df["alpha"], errors="coerce")

    # alphaê°€ NaNì¸ í–‰ ì œê±°
    df = df.dropna(subset=["alpha"])

    # --- 2ë‹¨ê³„: í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸° ìƒì„± ---
    preview_lines = []
    for _, r in df.iterrows():
        nm = r["to_name"] if r["to_name"] else "-"
        preview_lines.append(f"{r['from']} -> {r['to']}({nm}) : {float(r['alpha'])*100:.4f}%")

    # fromë³„ í•©/ì”ì—¬
    summary_lines = []
    grouped = df.groupby("from")["alpha"].sum()
    for origin_code, total_alpha in grouped.items():
        remaining = 1.0 - float(total_alpha)
        summary_lines.append(
            f"[from={origin_code}] ì´ë™í•©={float(total_alpha)*100:.4f}%, ì”ì—¬={remaining*100:.4f}%"
        )

    return df, meta, preview_lines, summary_lines

### ì‚¬ìš©ì ì •ì˜ í•¨ìˆ˜ ì„ ì–¸
def make_binary_matrix(matrix, threshold):
    # ì„ê³„ê°’ ì´í•˜ì˜ ì›ì†Œë“¤ì„ 0ìœ¼ë¡œ ì„¤ì •
    binary_matrix = matrix.apply(lambda x: np.where(x > threshold, 1, 0))
    return binary_matrix

def filter_matrix(matrix, threshold):
    # ì„ê³„ê°’ ì´í•˜ì˜ ì›ì†Œë“¤ì„ 0ìœ¼ë¡œ ì„¤ì •
    filtered_matrix = matrix.where(matrix > threshold, 0)
    return filtered_matrix

def calculate_network_centralities(G_bn, df_label, use_weight=False):
    weight_arg = 'weight' if use_weight else None

    # Degree
    in_degree_bn = dict(G_bn.in_degree(weight=weight_arg))
    out_degree_bn = dict(G_bn.out_degree(weight=weight_arg))

    df_degree = df_label.iloc[2:, :2].copy()
    df_degree["in_degree"] = pd.Series(in_degree_bn).sort_index().values.reshape(-1, 1)
    df_degree["out_degree"] = pd.Series(out_degree_bn).sort_index().values.reshape(-1, 1)

    gd_in_mean = df_degree["in_degree"].mean()
    gd_in_std = df_degree["in_degree"].std()
    gd_out_mean = df_degree["out_degree"].mean()
    gd_out_std = df_degree["out_degree"].std()

    # Betweenness
    bc_bn = nx.betweenness_centrality(G_bn, normalized=False, endpoints=False, weight=weight_arg)
    num_n = len(G_bn)
    bc_bn = {node: value / (num_n * (num_n - 1)) for node, value in bc_bn.items()}

    df_bc = df_label.iloc[2:, :2].copy()
    df_bc["Betweenness Centrality"] = pd.Series(bc_bn).sort_index().values.reshape(-1, 1)

    bc_mean = df_bc["Betweenness Centrality"].mean()
    bc_std = df_bc["Betweenness Centrality"].std()

    # Closeness
    cci_bn = nx.closeness_centrality(G_bn, distance=weight_arg)
    cco_bn = nx.closeness_centrality(G_bn.reverse(), distance=weight_arg)

    df_cc = df_label.iloc[2:, :2].copy()
    df_cc["Indegree_Closeness Centrality"] = pd.Series(cci_bn).sort_index().values.reshape(-1, 1)
    df_cc["Outdegree_Closeness Centrality"] = pd.Series(cco_bn).sort_index().values.reshape(-1, 1)

    cc_in_mean = df_cc["Indegree_Closeness Centrality"].mean()
    cc_in_std = df_cc["Indegree_Closeness Centrality"].std()
    cc_out_mean = df_cc["Outdegree_Closeness Centrality"].mean()
    cc_out_std = df_cc["Outdegree_Closeness Centrality"].std()

    # Eigenvector
    evi_bn = nx.eigenvector_centrality(G_bn, max_iter=500, tol=1e-06, weight=weight_arg)
    evo_bn = nx.eigenvector_centrality(G_bn.reverse(), max_iter=500, tol=1e-06, weight=weight_arg)

    df_ev = df_label.iloc[2:, :2].copy()
    df_ev["Indegree_Eigenvector Centrality"] = pd.Series(evi_bn).sort_index().values.reshape(-1, 1)
    df_ev["Outdegree_Eigenvector Centrality"] = pd.Series(evo_bn).sort_index().values.reshape(-1, 1)

    ev_in_mean = df_ev["Indegree_Eigenvector Centrality"].mean()
    ev_in_std = df_ev["Indegree_Eigenvector Centrality"].std()
    ev_out_mean = df_ev["Outdegree_Eigenvector Centrality"].mean()
    ev_out_std = df_ev["Outdegree_Eigenvector Centrality"].std()

    # HITS (ê°€ì¤‘ì¹˜ ì§€ì› ì•ˆ í•¨ â†’ ê·¸ëŒ€ë¡œ ì‚¬ìš©)
    hubs, authorities = nx.hits(G_bn, max_iter=1000, tol=1e-08, normalized=True)

    df_hi = df_label.iloc[2:, :2].copy()
    df_hi["HITS Hubs"] = pd.Series(hubs).sort_index().values.reshape(-1, 1)
    df_hi["HITS Authorities"] = pd.Series(authorities).sort_index().values.reshape(-1, 1)

    hi_hub_mean = df_hi["HITS Hubs"].mean()
    hi_hub_std = df_hi["HITS Hubs"].std()
    hi_ah_mean = df_hi["HITS Authorities"].mean()
    hi_ah_std = df_hi["HITS Authorities"].std()

    # Structural Hole Metrics (Constraint & Efficiency)
    constraints, efficiencies = calculate_kim_metrics(G_bn, weight=weight_arg)
    df_kim = df_label.iloc[2:, :2].copy()
    df_kim["Constraint"] = pd.Series(constraints).sort_index().values.reshape(-1, 1)
    df_kim["Efficiency"] = pd.Series(efficiencies).sort_index().values.reshape(-1, 1)

    # í‰ê· (Mean) ë° í‘œì¤€í¸ì°¨(Std) ê³„ì‚°
    kim_const_mean = df_kim["Constraint"].mean()
    kim_const_std = df_kim["Constraint"].std()
    kim_eff_mean = df_kim["Efficiency"].mean()
    kim_eff_std = df_kim["Efficiency"].std()

    return (
        df_degree, df_bc, df_cc, df_ev, df_hi, df_kim,  # df_kim ì¶”ê°€
        gd_in_mean, gd_in_std, gd_out_mean, gd_out_std,
        bc_mean, bc_std,
        cc_in_mean, cc_in_std, cc_out_mean, cc_out_std,
        ev_in_mean, ev_in_std, ev_out_mean, ev_out_std,
        hi_hub_mean, hi_hub_std, hi_ah_mean, hi_ah_std,
        kim_const_mean, kim_const_std, kim_eff_mean, kim_eff_std  # í†µê³„ì¹˜ 4ê°œ ì¶”ê°€
    )

@st.cache_data()
def get_submatrix_withlabel(df, start_row, start_col, end_row, end_col, first_index_of_df, numberoflabel = 2):
    row_indexs = list(range(first_index_of_df[0]-numberoflabel, first_index_of_df[0])) + list(range(start_row, end_row+1))
    col_indexs = list(range(first_index_of_df[1]-numberoflabel, first_index_of_df[1])) + list(range(start_col, end_col+1))
    # print(row_indexs)
    # print(col_indexs)

    submatrix_withlabel = df.iloc[row_indexs, col_indexs]
    return submatrix_withlabel

def reduce_negative_values(df, first_idx, mid_ID_idx):
    # ë°ì´í„°í”„ë ˆì„ ë³µì‚¬
    df_editing = df.copy()

    # first_idxì—ì„œ mid_ID_idxê¹Œì§€ì˜ ë²”ìœ„ ìŠ¬ë¼ì´ì‹±
    df_test = df_editing.iloc[first_idx[0]:mid_ID_idx[0], first_idx[1]:mid_ID_idx[1]].apply(pd.to_numeric, errors='coerce')

    # ìŒìˆ˜ ê°’ì´ ìˆëŠ” ìœ„ì¹˜ ì¶”ì  ë° ì¤„ì¸ ê°’ ê³„ì‚°
    reduced_values_per_column = {}

    def reduce_and_track(value, col_index):
        if value < 0:
            # ì¤„ì¼ ê°’ ì €ì¥ (ìŒìˆ˜ ê°’ì˜ ì ˆë°˜)
            reduced_value = value / 2
            if col_index not in reduced_values_per_column:
                reduced_values_per_column[col_index] = 0
            reduced_values_per_column[col_index] += value - reduced_value  # ì›ë˜ ê°’ - ì ˆë°˜ìœ¼ë¡œ ì¤„ì¸ ê°’
            return reduced_value
        return value

    # ìŒìˆ˜ì¸ ê°’ë§Œ 1/2ë¡œ ì¤„ì´ë©´ì„œ ì¶”ì 
    for col_idx in range(df_test.shape[1]):
        df_test.iloc[:, col_idx] = df_test.iloc[:, col_idx].apply(lambda x: reduce_and_track(x, col_idx))

    # ìˆ˜ì •ëœ ê°’ì„ ì›ë³¸ ë°ì´í„°í”„ë ˆì„ì— ë‹¤ì‹œ ë°˜ì˜ (first_idxì—ì„œ mid_ID_idxê¹Œì§€ì˜ ë¶€ë¶„)
    df_editing.iloc[first_idx[0]:mid_ID_idx[0], first_idx[1]:mid_ID_idx[1]] = df_test

    # ë§ˆì§€ë§‰ í–‰ì— ì¤„ì¸ ê°’ë§Œí¼ ë”í•˜ê¸°
    last_row_index = df_editing.shape[0] - 1
    for col_idx, total_reduced in reduced_values_per_column.items():
        df_editing.iloc[last_row_index, first_idx[1] + col_idx] -= total_reduced

    msg = "ìŒìˆ˜ ê°’ë“¤ì„ ì ˆë°˜ìœ¼ë¡œ ì¤„ì´ê³ , ì¤„ì¸ ê°’ì„ ë§ˆì§€ë§‰ í–‰ì— ë”í–ˆìŠµë‹ˆë‹¤."

    # ì¤‘ê°„ ì¸ë±ìŠ¤ ê°’ì€ ê·¸ëŒ€ë¡œ ë°˜í™˜ (mid_ID_idxëŠ” í–‰ê³¼ ì—´ ì¸ë±ìŠ¤ì´ë¯€ë¡œ ì´ ê²½ìš° ë³€ê²½ë˜ì§€ ì•ŠìŒ)
    return df_editing, msg, mid_ID_idx




def get_mid_ID_idx(df, first_idx):
    matrix_X = df.iloc[first_idx[0]:, first_idx[1]:].astype(float)
    row_cnt, col_cnt, row_sum, col_sum = 0, 0, 0, 0
    for v in matrix_X.iloc[0]:
        if abs(row_sum - v) < 0.001:
            if v == 0:
                continue
            else: break
        row_cnt += 1
        row_sum += v
    for v in matrix_X.iloc[:, 0]:
        print(f'gap: {col_sum-v}, sum: {col_sum}, value: {v}')
        if abs(col_sum - v) < 0.001:
            if v == 0:
                continue
            else: break
        col_cnt += 1
        col_sum += v
    
    if row_cnt == col_cnt:
        size = row_cnt
    else:
        size = max(row_cnt, col_cnt)

    return (first_idx[0]+size, first_idx[1]+size)

def insert_row_and_col(df, first_idx, mid_ID_idx, code, name, num_of_label):
    df_editing = df.copy()
    df_editing.insert(loc=mid_ID_idx[1], column='a', value=np.nan, allow_duplicates=True)
    df_editing.iloc[first_idx[0]-num_of_label, mid_ID_idx[1]] = code
    df_editing.iloc[first_idx[0]-num_of_label+1, mid_ID_idx[1]] = name
    df_editing.iloc[first_idx[0]:, mid_ID_idx[1]] = 0
    df_editing.columns = range(df_editing.shape[1])
    df_editing = df_editing.T   
    df_editing.insert(loc=mid_ID_idx[0], column='a', value=np.nan, allow_duplicates=True)
    df_editing.iloc[first_idx[1]-num_of_label, mid_ID_idx[0]] = code
    df_editing.iloc[first_idx[1]-num_of_label+1, mid_ID_idx[0]] = name
    df_editing.iloc[first_idx[1]:, mid_ID_idx[0]] = 0
    df_editing.columns = range(df_editing.shape[1])
    df_editing = df_editing.T
    df_inserted = df_editing.copy()
    mid_ID_idx = (mid_ID_idx[0]+1, mid_ID_idx[1]+1)
    msg = f'A new row and column (Code: {code}, Name: {name}) have been inserted.'

    return df_inserted, mid_ID_idx, msg

def transfer_to_new_sector(df, first_idx, origin_code, target_code, ratio, code_label = 2):
    df_editing = df.copy()
    target_idx = df_editing.index[df_editing[first_idx[1]-code_label] == target_code].tolist()
    if len(target_idx) == 1:
        target_idx = target_idx[0]
    else:
        msg = 'ERROR: target code is not unique.'
        return df_editing, msg
    origin_idx = df_editing.index[df_editing[first_idx[1]-code_label] == origin_code].tolist()
    if len(origin_idx) == 1:
        origin_idx = origin_idx[0]
    else:
        msg = 'ERROR: origin code is not unique.'
        return df_editing, msg
    df_editing.iloc[first_idx[0]:, first_idx[1]:] = df_editing.iloc[first_idx[0]:, first_idx[1]:].apply(pd.to_numeric, errors='coerce')
    origin_idx = (origin_idx, origin_idx-first_idx[0]+first_idx[1])
    target_idx = (target_idx, target_idx-first_idx[0]+first_idx[1])
    df_editing.iloc[target_idx[0] ,first_idx[1]:] += df_editing.iloc[origin_idx[0] ,first_idx[1]:] * ratio
    df_editing.iloc[origin_idx[0] ,first_idx[1]:] = df_editing.iloc[origin_idx[0] ,first_idx[1]:] * (1-ratio)
    df_editing.iloc[first_idx[0]: ,target_idx[1]] += df_editing.iloc[first_idx[0]: ,origin_idx[1]] * ratio
    df_editing.iloc[first_idx[0]: ,origin_idx[1]] = df_editing.iloc[first_idx[0]: ,origin_idx[1]] * (1-ratio)

    msg = f'{ratio*100}% of {origin_code} has been moved to {target_code}.'
    return df_editing, msg

def remove_zero_series(df, first_idx, mid_ID_idx):
    df_editing = df.copy()
    df_test = df_editing.copy()
    df_test = df_editing.iloc[first_idx[0]:, first_idx[1]:].apply(pd.to_numeric, errors='coerce')
    zero_row_indices = df_test.index[(df_test == 0).all(axis=1)].tolist()
    zero_row_indices = [item for item in zero_row_indices if item>=first_idx[0] and item<=mid_ID_idx[0]]
    zero_col_indices = list(map(lambda x: x - first_idx[0] + first_idx[1], zero_row_indices))
    df_editing.drop(zero_row_indices, inplace=True)
    df_editing.drop(zero_col_indices, inplace=True, axis=1)
    df_editing.columns = range(df_editing.shape[1])
    df_editing.index = range(df_editing.shape[0])
    count = len(zero_col_indices)
    msg = f'{count}ê°œì˜ í–‰(ì—´)ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.'
    mid_ID_idx = (mid_ID_idx[0] - count, mid_ID_idx[1] - count)
    return df_editing, msg, mid_ID_idx

def donwload_data(df, file_name):
    csv = convert_df(df)
    button = st.download_button(label=f"{file_name} ë‹¤ìš´ë¡œë“œ", data=csv, file_name=file_name+".csv", mime='text/csv')
    return button




@st.cache_data()
def load_data(file):
    st.session_state['df'] = pd.read_excel(file, header=None)
    return st.session_state['df']

@st.cache_data 
def convert_df(df):
    return df.to_csv(header=False, index=False).encode('utf-8-sig')


@st.cache_data
def make_zip_bytes(dfs: dict[str, pd.DataFrame]) -> bytes:
    """
    dfs: dict where keys are desired CSV filenames and values are DataFrames.
    ë°˜í™˜ê°’: ZIP íŒŒì¼ì˜ ë°”ì´ë„ˆë¦¬
    """
    buf = io.BytesIO()
    with zipfile.ZipFile(buf, "w", compression=zipfile.ZIP_DEFLATED) as zf:
        for fname, df in dfs.items():
            csv_bytes = df.to_csv(index=False).encode("utf-8-sig")
            zf.writestr(f"{fname}.csv", csv_bytes)
    return buf.getvalue()

def download_multiple_csvs_as_zip(dfs: dict[str, pd.DataFrame], zip_name: str):
    zip_bytes = make_zip_bytes(dfs)
    return st.download_button(
        label=f"{zip_name} ë‹¤ìš´ë¡œë“œ",
        data=zip_bytes,
        file_name=f"{zip_name}.zip",
        mime="application/zip",
    )

def compute_leontief_inverse(A, epsilon=0.05, max_iter=100):
    """
    Leontief ì—­í–‰ë ¬ì„ ë¬´í•œê¸‰ìˆ˜(I + A + A^2 + ...)ë¡œ ê·¼ì‚¬ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜.
    ìˆ˜ë ´ ì¡°ê±´: ëˆ„ì í•©ì˜ ìƒëŒ€ë³€í™”ê°€ epsilon ì´í•˜ê°€ ë  ë•Œê¹Œì§€ ë°˜ë³µ.
    
    Parameters:
        A (ndarray): íˆ¬ì…ê³„ìˆ˜í–‰ë ¬.
        epsilon (float): ìˆ˜ë ´ íŒì • ê¸°ì¤€ (ì˜ˆ: 0.05 = 5%).
        max_iter (int): ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ (ë¬´í•œê¸‰ìˆ˜ì˜ ìˆ˜ë ´ì´ ì•ˆ ë  ê²½ìš° ëŒ€ë¹„).
    
    Returns:
        M (ndarray): I + A + A^2 + ... + A^r (rë²ˆì§¸ í•­ê¹Œì§€ ê³„ì‚°í•œ ê·¼ì‚¬ Leontief ì—­í–‰ë ¬).
    """
    n = A.shape[0]
    I = np.eye(n)           # n x n í•­ë“±í–‰ë ¬ ìƒì„±
    M = I.copy()            # ì´ˆê¸° ëˆ„ì í•©: M(0) = I
    s_prev = np.sum(M)      # ì´ˆê¸° ì „ì²´í•© (s(0))
    k = 1                   # ê±°ë“­ì œê³± ì°¨ìˆ˜ ì´ˆê¸°í™”

    while k < max_iter:
        # A^k ê³„ì‚° (í–‰ë ¬ì˜ ê±°ë“­ì œê³±)
        A_power = np.linalg.matrix_power(A, k)
        
        # ëˆ„ì í•© ì—…ë°ì´íŠ¸: M(k) = M(k-1) + A^k
        M_new = M + A_power
        
        # ìƒˆë¡œìš´ ì „ì²´í•© ê³„ì‚°
        s_new = np.sum(M_new)
        
        # ìƒëŒ€ ë³€í™”ëŸ‰ ê³„ì‚°: (s(k) - s(k-1)) / s(k-1)
        ratio_change = (s_new - s_prev) / s_prev if s_prev != 0 else 0
        
        # ì¤‘ê°„ ê²°ê³¼ ì¶œë ¥ (ë””ë²„ê·¸ìš©)
        print(f"Iteration {k}: ratio_change = {ratio_change:.4f}")
        
        # ìˆ˜ë ´ íŒì •: ìƒëŒ€ ë³€í™”ê°€ epsilon ì´í•˜ì´ë©´ ì¢…ë£Œ
        if ratio_change <= epsilon:
            M = M_new
            break
        
        # ì—…ë°ì´íŠ¸ í›„ ë‹¤ìŒ ë°˜ë³µ ì§„í–‰
        M = M_new
        s_prev = s_new
        k += 1
    
    return M

def separate_diagonals(N0):
    """
    ì…ë ¥ í–‰ë ¬ N0ì—ì„œ ëŒ€ê°ì›ì†Œì™€ ë¹„ëŒ€ê°ì›ì†Œ(ë„¤íŠ¸ì›Œí¬ base)ë¥¼ ë¶„ë¦¬.
    
    Parameters:
        N0 (ndarray): Leontief ì—­í–‰ë ¬ ê·¼ì‚¬ (I + A + A^2 + ...).
    
    Returns:
        Diagon (ndarray): N0ì—ì„œ ëŒ€ê°ì›ì†Œë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ë¥¼ 0ìœ¼ë¡œ ë§Œë“  í–‰ë ¬.
        N (ndarray): N0ì—ì„œ ëŒ€ê°ì›ì†Œë¥¼ ëª¨ë‘ 0ìœ¼ë¡œ ë§Œë“  ë„¤íŠ¸ì›Œí¬ í–‰ë ¬.
    """
    # np.diag: ëŒ€ê° ì„±ë¶„ ì¶”ì¶œ, np.diagflat: ëŒ€ê° í–‰ë ¬ë¡œ ì¬êµ¬ì„±
    Diagon = np.diag(np.diag(N0))
    N = N0 - Diagon
    return Diagon, N

def threshold_network(N, delta):
    """
    ë„¤íŠ¸ì›Œí¬ í–‰ë ¬ Nì—ì„œ ì„ê³„ì¹˜ deltaë³´ë‹¤ ì‘ì€ ê°’ë“¤ì„ 0ìœ¼ë¡œ ëŒ€ì²´.
    
    Parameters:
        N (ndarray): ì›ë³¸ ë„¤íŠ¸ì›Œí¬ í–‰ë ¬.
        delta (float): ì„ê³„ì¹˜ ê°’.
    
    Returns:
        N_thresholded (ndarray): thresholding ì ìš©ëœ ë„¤íŠ¸ì›Œí¬ í–‰ë ¬.
    """
    N_thresholded = N.copy()
    N_thresholded[N_thresholded < delta] = 0
    return N_thresholded

def create_binary_network(N):
    """
    ê°€ì¤‘ì¹˜ ë„¤íŠ¸ì›Œí¬ í–‰ë ¬ Në¥¼ ì´ì§„(0-1) ë„¤íŠ¸ì›Œí¬ë¡œ ë³€í™˜ (ì–‘ìˆ˜ë©´ 1, ì•„ë‹ˆë©´ 0).
    
    Parameters:
        N (ndarray): ê°€ì¤‘ì¹˜ ë„¤íŠ¸ì›Œí¬ í–‰ë ¬.
    
    Returns:
        BN (ndarray): ì´ì§„í™”ëœ ë„¤íŠ¸ì›Œí¬ (ë°©í–¥ì„± ìœ ì§€).
    """
    BN = (N > 0).astype(int)
    return BN

def create_undirected_network(BN):
    """
    ë°©í–¥ì„±ì´ ìˆëŠ” ì´ì§„ ë„¤íŠ¸ì›Œí¬ BNë¥¼ ë¬´ë°©í–¥ ë„¤íŠ¸ì›Œí¬ë¡œ ë³€í™˜.
    ë‘ ë…¸ë“œ ê°„ ì–´ëŠ í•œìª½ì´ë¼ë„ ì—°ê²°ë˜ì–´ ìˆìœ¼ë©´, ë¬´ë°©í–¥ ì—°ê²°ë¡œ ì²˜ë¦¬.
    
    Parameters:
        BN (ndarray): ì´ì§„í™”ëœ ë°©í–¥ì„± ë„¤íŠ¸ì›Œí¬.
    
    Returns:
        UN (ndarray): ë¬´ë°©í–¥(ëŒ€ì¹­) ì´ì§„ ë„¤íŠ¸ì›Œí¬.
    """
    UN = ((BN + BN.T) > 0).astype(int)
    return UN

@st.cache_data()
def threshold_count(matrix):
    """
    [Integration Logic]
    1. Method 2 (Derivative): ë³€í™”ìœ¨ ì•ˆì •í™” ì§€ì  ê³„ì‚° (ê¸°ì¡´ ìœ ì§€)
    2. Method 2-1 (Distance): ì›ì  ê±°ë¦¬ ìµœì†Œí™” ì§€ì  ê³„ì‚° (ê¸°ì¡´ ìœ ì§€ - ì‹œì‘ì  ì—­í• )
    3. Connectivity Check: Method 2-1 ì§€ì ì—ì„œ ê³ ë¦½ ë…¸ë“œ ë°œìƒ ì‹œ, ì‚¬ë¼ì§ˆ ë•Œê¹Œì§€ Threshold í•˜í–¥ ì¡°ì • (ì‹ ê·œ ì¶”ê°€)
    """
    # -------------------------------------------------------------------------
    # 0. ë°ì´í„° ì¤€ë¹„
    # -------------------------------------------------------------------------
    if hasattr(matrix, 'to_numpy'):
        mat_data = matrix.to_numpy()
    else:
        mat_data = np.array(matrix)
        
    mat_data = mat_data.copy().astype(float)
    np.fill_diagonal(mat_data, 0) # ëŒ€ê° ì„±ë¶„ ì œì™¸
    
    N = mat_data.shape[0]
    total_elements = N**2 - N
    
    # xì¶• ì„¤ì •
    delta = 0.01
    max_val = np.max(mat_data)
    x_values = np.arange(0, max_val + delta, delta)
    
    # -------------------------------------------------------------------------
    # 1. ì§€í‘œ ê³„ì‚°: y(ìƒì¡´ìœ¨) & w(ë³€í™”ìœ¨)
    # -------------------------------------------------------------------------
    # y: Survival Ratio
    y_list = []
    for x in x_values:
        count = (mat_data >= x).sum()
        ratio = count / total_elements
        y_list.append(ratio)
    y = np.array(y_list)

    # w: Slope Change Rate (Method 2)
    if len(y) > 1:
        z = (y[1:] - y[:-1]) / delta
    else:
        z = np.zeros(len(y))

    w_list = []
    w_x_values = []
    for i in range(1, len(z)):
        val_w = abs(z[i] - z[i-1]) / delta 
        w_list.append(val_w)
        if i+1 < len(x_values):
            w_x_values.append(x_values[i+1])
    w = np.array(w_list)
    w_x_values = np.array(w_x_values)
    
    # Method 2: Stability Check (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
    epsilon = 0.01
    opt_idx_method2 = 0
    found_method2 = False
    
    for k in range(1, len(w)):
        if k > 3 and (w[k-1] - w[k]) <= epsilon:
            opt_idx_method2 = k + 2
            found_method2 = True
            break
    if not found_method2 and len(x_values) > 0:
        opt_idx_method2 = len(x_values) - 1
    
    threshold_method2 = x_values[opt_idx_method2] if len(x_values) > opt_idx_method2 else 0

    # -------------------------------------------------------------------------
    # 2. Method 2-1 (Distance Minimization) - [ê¸°ì¤€ì ]
    # -------------------------------------------------------------------------
    dist_sq = x_values**2 + y**2
    opt_idx_dist = np.argmin(dist_sq)
    
    threshold_dist = x_values[opt_idx_dist]
    min_y = y[opt_idx_dist] if len(y) > opt_idx_dist else 0

    # -------------------------------------------------------------------------
    # 3. [Logic Addition] Connectivity Backtracking
    # Method 2-1 ì§€ì (opt_idx_dist)ì—ì„œ ì‹œì‘í•˜ì—¬ 0ë°©í–¥ìœ¼ë¡œ ìŠ¤ìº”
    # -------------------------------------------------------------------------
    final_idx = opt_idx_dist
    adjusted = False
    
    # í˜„ì¬ ìµœì ì (Distance Min)ë¶€í„° 0ê¹Œì§€ ì—­ìˆœ íƒìƒ‰
    for idx in range(opt_idx_dist, -1, -1):
        t = x_values[idx]
        
        # Binary Masking
        mask = (mat_data >= t) # 1 if connected, else 0
        
        # ê³ ë¦½ ë…¸ë“œ ì²´í¬ (Undirected ê´€ì : In-degree + Out-degree == 0 ì´ë©´ ê³ ë¦½)
        # mask í–‰ë ¬ì—ì„œ í–‰ì˜ í•©(Out) + ì—´ì˜ í•©(In) ê³„ì‚°
        degrees = mask.sum(axis=1) + mask.sum(axis=0)
        
        if np.any(degrees == 0):
            # ê³ ë¦½ ë…¸ë“œê°€ ì¡´ì¬í•¨ -> Thresholdê°€ ë„ˆë¬´ ë†’ìŒ -> ê³„ì† ë‚®ì¶¤(Loop Continue)
            continue
        else:
            # ê³ ë¦½ ë…¸ë“œ ì—†ìŒ (All Connected) -> ë©ˆì¶¤
            final_idx = idx
            if idx < opt_idx_dist:
                adjusted = True
            break
    
    final_threshold = x_values[final_idx]
    final_y = y[final_idx] if len(y) > final_idx else 0

    # -------------------------------------------------------------------------
    # 4. ì‹œê°í™” (ëª¨ë“  ì§€í‘œ í¬í•¨)
    # -------------------------------------------------------------------------
    fig, ax1 = plt.subplots(figsize=(10, 7))

    # [ì™¼ìª½ ì¶•] y(x) Curve
    color1 = 'tab:blue'
    ax1.set_xlabel('Threshold (x)')
    ax1.set_ylabel('Survival Ratio (y)', color=color1, fontweight='bold')
    ax1.plot(x_values, y, color=color1, label='y: Survival Ratio', linewidth=2, alpha=0.7)
    ax1.tick_params(axis='y', labelcolor=color1)
    ax1.grid(True, alpha=0.3)
    
    # [ì˜¤ë¥¸ìª½ ì¶•] w(t) Curve (ê¸°ì¡´ Method 2 ì‹œê°í™” ìœ ì§€)
    if len(w) > 0:
        ax2 = ax1.twinx()
        color2 = 'tab:orange'
        ax2.set_ylabel('Slope Change Rate (w)', color=color2, fontweight='bold')
        ax2.plot(w_x_values, w, color=color2, linestyle='--', alpha=0.5, label='w: Slope Stability')
        ax2.tick_params(axis='y', labelcolor=color2)

    # [Indicator 1] Method 2 (Stability) - íšŒìƒ‰ ìˆ˜ì§ì„ 
    ax1.axvline(x=threshold_method2, color='gray', linestyle='-.', alpha=0.6,
                label=f'Method 2 (Stable): {threshold_method2:.4f}')

    # [Indicator 2] Method 2-1 (Distance Min) - ë¹¨ê°„ ì  (ì›ë˜ì˜ ìˆ˜í•™ì  ìµœì ì )
    ax1.plot(threshold_dist, min_y, 'ro', markersize=8, alpha=0.6,
             label=f'Method 2-1 (Dist Min): {threshold_dist:.4f}')

    # [Indicator 3] Final Decision (No Isolated) - ì´ˆë¡ìƒ‰ ë³„/X (ìµœì¢… ê²°ì •)
    # ì¡°ì •ì´ ë°œìƒí–ˆë‹¤ë©´ í™”ì‚´í‘œì™€ í•¨ê»˜ í‘œì‹œ
    label_final = f'Final (No Isolated): {final_threshold:.4f}'
    
    if adjusted:
        # ì¡°ì •ëœ ê²½ìš°: Method 2-1 -> Final ë¡œ í™”ì‚´í‘œ í‘œì‹œ
        ax1.annotate('', xy=(final_threshold, final_y), xytext=(threshold_dist, min_y),
                     arrowprops=dict(arrowstyle="->", color='red', lw=2))
        ax1.plot(final_threshold, final_y, 'X', color='red', markersize=12, zorder=10, label=label_final)
    else:
        # ì¡°ì • ì•ˆ ëœ ê²½ìš°: ë¹¨ê°„ ì  ìœ„ì— ì´ˆë¡ìƒ‰ í…Œë‘ë¦¬ ë“±ì„ ì”Œì›Œ ê°•ì¡°
        ax1.plot(final_threshold, final_y, 'g*', markersize=14, zorder=10, label=label_final)

    # ë²”ë¡€ í†µí•©
    lines1, labels1 = ax1.get_legend_handles_labels()
    if len(w) > 0:
        lines2, labels2 = ax2.get_legend_handles_labels()
        ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper right')
    else:
        ax1.legend(loc='upper right')

    plt.title('Threshold Optimization: Distance Min + Connectivity Check')
    fig.tight_layout()
    st.pyplot(fig)
    
    # -------------------------------------------------------------------------
    # 5. ê²°ê³¼ ë°˜í™˜ ë° ì„¤ëª…
    # -------------------------------------------------------------------------
    msg_adjustment = ""
    if adjusted:
        msg_adjustment = f"âš ï¸ ìˆ˜í•™ì  ìµœì ì (`{threshold_dist:.4f}`)ì—ì„œ ê³ ë¦½ ë…¸ë“œê°€ ë°œê²¬ë˜ì–´, `{final_threshold:.4f}`ë¡œ í•˜í–¥ ì¡°ì •í–ˆìŠµë‹ˆë‹¤."
    else:
        msg_adjustment = f"âœ… ìˆ˜í•™ì  ìµœì ì (`{threshold_dist:.4f}`)ì´ ê³ ë¦½ ë…¸ë“œ ì—†ì´ ì•ˆì •ì ì…ë‹ˆë‹¤."

    st.markdown(f"""
    **ìµœì  ì„ê³„ê°’ ë¶„ì„ ê²°ê³¼**
    - **Method 2 (Stability)**: `{threshold_method2:.4f}`
    - **Method 2-1 (Dist Min)**: `{threshold_dist:.4f}` (Backtracking ì‹œì‘ì )
    - **Final Decision**: `{final_threshold:.4f}`
    
    {msg_adjustment}
    """)
    
    return final_threshold

def calculate_kim_metrics(G, weight='weight'):
    """
    Kim (2021) ë°©ì‹ì˜ Constraintì™€ Efficiencyë¥¼ ê³„ì‚°í•˜ì—¬ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜
    Return: (constraints_dict, efficiencies_dict)
    """
    # 1. Constraint (Burt's constraint)
    # ê°€ì¤‘ì¹˜ê°€ ìˆìœ¼ë©´ ìƒì‚°ìœ ë°œê³„ìˆ˜ ë“±ì„ ë°˜ì˜
    constraints = nx.constraint(G, weight=weight)
    
    # 2. Efficiency (Kim's redundancy-based)
    efficiencies = {}
    nodes = list(G.nodes())
    
    # íš¨ìœ¨ì„± ê³„ì‚°ì„ ìœ„í•œ ì‚¬ì „ ê³„ì‚° (ì†ë„ ìµœì í™”)
    # ì–‘ë°©í–¥ ê±°ë˜ëŸ‰(volume) ê³„ì‚° í—¬í¼
    def get_vol(u, v):
        if not G.has_edge(u, v): return 0.0
        return G[u][v].get(weight, 1.0) if weight else 1.0

    def get_bi_vol(u, v):
        return get_vol(u, v) + get_vol(v, u)

    node_total_volumes = {} # ë¶„ëª¨: (In + Out sum)
    node_max_volumes = {}   # ë¶„ëª¨: Max connection strength
    
    for n in nodes:
        # Total Volume (In + Out)
        vol_in = G.in_degree(n, weight=weight)
        vol_out = G.out_degree(n, weight=weight)
        node_total_volumes[n] = vol_in + vol_out
        
        # Max Volume with any partner
        partners = set(G.predecessors(n)) | set(G.successors(n))
        max_vol = 0.0
        for p in partners:
            vol = get_bi_vol(n, p)
            if vol > max_vol:
                max_vol = vol
        node_max_volumes[n] = max_vol

    # ê°œë³„ ë…¸ë“œ íš¨ìœ¨ì„± ê³„ì‚°
    for i in nodes:
        partners_i = list(set(G.predecessors(i)) | set(G.successors(i)))
        Ni = len(partners_i)
        
        if Ni == 0:
            efficiencies[i] = 0.0
            continue
            
        sum_Rij = 0.0
        for j in partners_i:
            # jì™€ ië¥¼ ì œì™¸í•œ ì œ3ì(q) íƒìƒ‰ (Redundancy check)
            potential_qs = [q for q in partners_i if q != j and q != i]
            
            R_ij = 0.0
            for q in potential_qs:
                # rho_iq: iì˜ ì „ì²´ ê±°ë˜ ì¤‘ qì™€ì˜ ë¹„ì¤‘
                vol_iq = get_bi_vol(i, q)
                denom_i = node_total_volumes.get(i, 0)
                rho_iq = vol_iq / denom_i if denom_i > 1e-9 else 0.0
                
                # tau_jq: jì˜ ìµœëŒ€ ê±°ë˜ ëŒ€ë¹„ qì™€ì˜ ê°•ë„
                vol_jq = get_bi_vol(j, q)
                max_vol_j = node_max_volumes.get(j, 0)
                tau_jq = vol_jq / max_vol_j if max_vol_j > 1e-9 else 0.0
                
                R_ij += (rho_iq * tau_jq)
            sum_Rij += R_ij
        
        # Kim's Efficiency Formula: epsilon = T_i / N_i where T_i = N_i - sum(R_ij)
        Ti = Ni - sum_Rij
        efficiencies[i] = Ti / Ni if Ni > 0 else 0.0
        
    return constraints, efficiencies

def calculate_standard_metrics(G_directed, weight='weight'):
    """Burt í‘œì¤€ ë°©ì‹ (Efficiency = Effective Size / Out-Degree)"""
    std_constraints = nx.constraint(G_directed, weight=weight)
    effective_sizes = nx.effective_size(G_directed, weight=weight)
    
    std_efficiencies = {}
    for n, eff_size in effective_sizes.items():
        degree = G_directed.out_degree(n) # Standard Burt uses Out-degree for ego network size
        if degree > 0:
            std_efficiencies[n] = eff_size / degree
        else:
            std_efficiencies[n] = 0.0
            
    return std_constraints, std_efficiencies


@st.cache_data
def extract_network_method_b(matrix):
    """
    Method A: ë¬´í•œê¸‰ìˆ˜(Infinite Series) í™•ì¥ì„ í†µí•œ ë„¤íŠ¸ì›Œí¬ ì¶”ì¶œ
    êµ¬ì¡°: threshold_count í•¨ìˆ˜ì™€ ë™ì¼í•œ íë¦„ (ê³„ì‚° -> ì‹œê°í™” -> ê²°ê³¼ë°˜í™˜)
    """
    # -------------------------------------------------------------------------
    # 0. ë°ì´í„° ì¤€ë¹„
    # -------------------------------------------------------------------------
    if hasattr(matrix, 'to_numpy'):
        mat_data = matrix.to_numpy()
    else:
        mat_data = np.array(matrix)
        
    A = mat_data.copy().astype(float)
    np.fill_diagonal(A, 0) # ëŒ€ê° ì„±ë¶„ 0 ì²˜ë¦¬
    
    n = A.shape[0]
    
    # íŒŒë¼ë¯¸í„° ì„¤ì • (Pseudo-code ê¸°ì¤€)
    epsilon = 0.1          # 10% ê¸°ì¤€
    max_iter = 20          # ë¬´í•œ ë£¨í”„ ë°©ì§€ìš© ì•ˆì „ ì¥ì¹˜
    
    # ì´ˆê¸°ê°’ (k=0)
    N_accum = np.zeros((n, n)) # N0
    s_accum = 0.0              # s0
    
    # ì‹œê°í™”ë¥¼ ìœ„í•œ ë¦¬ìŠ¤íŠ¸
    k_list = []
    ratio_list = []
    s_list = []

    # -------------------------------------------------------------------------
    # 1. Iteration: M(k) = A^k ë° Reduce ìˆ˜í–‰
    # -------------------------------------------------------------------------
    final_k = 0
    converged = False
    
    # këŠ” 1ë¶€í„° ì‹œì‘
    for k in range(1, max_iter + 1):
        # M(k) = A^k
        try:
            M_k = np.linalg.matrix_power(A, k)
        except:
            break # ìˆ˜ì¹˜ì  ë°œì‚° ë“± ì—ëŸ¬ ì‹œ ì¤‘ë‹¨

        # s(k) ê³„ì‚°: ëŒ€ê° ì„±ë¶„ ì œì™¸ ì›ì†Œ í•©
        off_diag_mask = ~np.eye(n, dtype=bool)
        vals = M_k[off_diag_mask]
        s_k = np.sum(vals)
        
        # av(k) ê³„ì‚°: í‰ê· 
        if (n*n - n) > 0:
            av_k = s_k / (n*n - n)
        else:
            av_k = 0
            
        # "M(k) reduce": av(k)ë³´ë‹¤ ì‘ì€ ì›ì†Œ 0 ì²˜ë¦¬ (Local Copy)
        M_k_reduced = np.where(M_k < av_k, 0, M_k)
        
        # Reduced ëœ ê°’ ê¸°ì¤€ìœ¼ë¡œ s(k) ì¬ê³„ì‚° (ëˆ„ì ì„ ìœ„í•´)
        vals_reduced = M_k_reduced[off_diag_mask]
        s_k_reduced = np.sum(vals_reduced)
        
        # ratio_change ê³„ì‚°
        # Pseudo-codeì˜ (s0 + s(k))/s0 ë…¼ë¦¬ëŠ” í•­ìƒ > 1 ì´ë¯€ë¡œ,
        # ìˆ˜ë ´ íŒë‹¨ì„ ìœ„í•´ 'ìƒˆë¡œ ì¶”ê°€ë˜ëŠ” ì •ë³´ëŸ‰ì˜ ë¹„ìœ¨' (s_k / s0)ë¡œ í•´ì„í•˜ì—¬ êµ¬í˜„
        if s_accum == 0:
            ratio_change = 1.0 # ì²« í„´ì€ ë¬´ì¡°ê±´ ì§„í–‰
        else:
            ratio_change = s_k_reduced / s_accum
            
        # ê¸°ë¡ ì €ì¥
        k_list.append(k)
        ratio_list.append(ratio_change)
        s_list.append(s_accum + s_k_reduced)
        
        # ëˆ„ì  ìˆ˜í–‰: N0 = N0 + M(k), s0 = s0 + s(k)
        N_accum = N_accum + M_k_reduced
        s_accum = s_accum + s_k_reduced
        final_k = k
        
        # ì¢…ë£Œ ì¡°ê±´ (Convergence Check)
        if k > 1 and ratio_change <= epsilon:
            converged = True
            break

    # -------------------------------------------------------------------------
    # 2. ì‹œê°í™” (Dual Axis: Change Ratio vs Total Info)
    # -------------------------------------------------------------------------
    fig, ax1 = plt.subplots(figsize=(10, 6))

    # [ì™¼ìª½ ì¶•] ë³€í™”ìœ¨ (Convergence Ratio)
    color1 = 'tab:red'
    ax1.set_xlabel('Iteration (k)')
    ax1.set_ylabel('Change Ratio (New/Total)', color=color1, fontweight='bold')
    ax1.plot(k_list, ratio_list, color=color1, marker='o', label='Ratio Change', linewidth=2)
    ax1.tick_params(axis='y', labelcolor=color1)
    ax1.grid(True, alpha=0.3)
    
    # Epsilon ê¸°ì¤€ì„ 
    ax1.axhline(y=epsilon, color='gray', linestyle='--', label=f'Epsilon ({epsilon})')

    # [ì˜¤ë¥¸ìª½ ì¶•] ëˆ„ì  ì •ë³´ëŸ‰ (Total Sum s0)
    ax2 = ax1.twinx()
    color2 = 'tab:blue'
    ax2.set_ylabel('Accumulated Signal (s0)', color=color2, fontweight='bold')
    ax2.plot(k_list, s_list, color=color2, linestyle='--', alpha=0.6, label='Total Signal (s0)')
    ax2.tick_params(axis='y', labelcolor=color2)

    # ë²”ë¡€ í•©ì¹˜ê¸°
    lines1, labels1 = ax1.get_legend_handles_labels()
    lines2, labels2 = ax2.get_legend_handles_labels()
    ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper right')

    plt.title(f'Method A Convergence: Stopped at k={final_k}')
    fig.tight_layout()
    st.pyplot(fig)
    
    # -------------------------------------------------------------------------
    # 3. ì‚¬ìš©ì ì„ íƒ UI / ê²°ê³¼ ì•ˆë‚´
    # -------------------------------------------------------------------------
    status_msg = "ìˆ˜ë ´ ì™„ë£Œ (Converged)" if converged else "ìµœëŒ€ ë°˜ë³µ ë„ë‹¬ (Max Iter)"
    
    st.markdown(f"""
    **Method A ì¶”ì¶œ ê²°ê³¼**
    - **ìµœì¢… ë°˜ë³µ íšŸìˆ˜ (k)**: `{final_k}` ({status_msg})
    - **ìµœì¢… ëˆ„ì  ì •ë³´ëŸ‰ (s0)**: `{s_accum:.4f}`
    - **ë§ˆì§€ë§‰ ë³€í™”ìœ¨**: `{ratio_list[-1]:.4f}` (ëª©í‘œ: $\le {epsilon}$)
    
    ğŸ’¡ **ì„¤ëª…:** í–‰ë ¬ì˜ ê±°ë“­ì œê³±($A^k$)ì„ í†µí•´ ê°„ì ‘ ì—°ê²°ì„ íƒìƒ‰í•˜ë©°, ì •ë³´ëŸ‰ ì¦ê°€ë¶„ì´ {epsilon*100}% ì´í•˜ê°€ ë  ë•Œê¹Œì§€ ë„¤íŠ¸ì›Œí¬ë¥¼ ëˆ„ì í–ˆìŠµë‹ˆë‹¤.
    """)
    
    # ì‚¬ìš©ìê°€ ì›í•˜ëŠ” network(í–‰ë ¬) ìì²´ë¥¼ ë°˜í™˜
    return N_accum